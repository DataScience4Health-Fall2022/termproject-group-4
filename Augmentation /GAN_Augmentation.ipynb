{"cells":[{"cell_type":"markdown","metadata":{"id":"AmPoRRtCZFYO"},"source":["# Model "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCImyvHGZnr6"},"outputs":[],"source":["import torch \n","import torch.nn as nn \n","import torch.optim as optim \n","import torchvision \n","import torchvision.datasets as datasets \n","import torchvision.transforms as transforms \n","from torch.utils.data import DataLoader \n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm \n","import cv2\n","from torchvision.utils import save_image\n","import os\n","import pandas as pd\n","import numpy as np "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41391,"status":"ok","timestamp":1667791856961,"user":{"displayName":"Gokul Srinivasan","userId":"01072835723997484871"},"user_tz":300},"id":"SYAR-Hv1ZInh","outputId":"5044548a-4ae3-437c-ec8f-e7ce184e2bb7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rsvf5kL1D-L2"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, channels_img, features_d, num_classes, img_size):\n","        super(Discriminator, self).__init__()\n","        self.img_size = img_size\n","        self.disc = nn.Sequential(\n","            # input: N x channels_img x 64 x 64\n","            nn.Conv2d(channels_img+1, features_d, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            # _block(in_channels, out_channels, kernel_size, stride, padding)\n","            self._block(features_d, features_d * 2, 4, 2, 1),\n","            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n","            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n","            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n","            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n","        )\n","        #this embedding is described as a visual stamp that the model can use to learn, basically, the class of the image\n","        self.embed = nn.Embedding(num_classes, img_size*img_size) \n","\n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","        return nn.Sequential(\n","            nn.Conv2d(\n","                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n","            ),\n","            nn.InstanceNorm2d(out_channels, affine=True),\n","            nn.LeakyReLU(0.2),\n","        )\n","\n","    def forward(self, x, labels):\n","        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size) #channel = 1 \n","        x = torch.cat([x, embedding], dim=1) #dims: N(batch-size) x img_channel(s) x H x W \n","        return self.disc(x)\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, channels_noise, channels_img, features_g, num_classes, img_size, embed_size):\n","        super(Generator, self).__init__()\n","        self.img_size = img_size\n","        self.net = nn.Sequential(\n","            # Input: N x channels_noise x 1 x 1\n","            self._block(channels_noise+embed_size, features_g * 16, 4, 1, 0),  # img: 4x4\n","            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n","            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n","            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n","            nn.ConvTranspose2d(\n","                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n","            ),\n","            # Output: N x channels_img x 64 x 64\n","            nn.Tanh(),\n","        )\n","        self.embed = nn.Embedding(num_classes, embed_size) #what is this class? \n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(\n","                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),\n","        )\n","\n","    def forward(self, x, labels):\n","        #latent vector z: N * noise_dim * 1 * 1\n","        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3) #what does unsqueeze do? \n","        x = torch.cat([x, embedding], dim=1)\n","        return self.net(x)\n","\n","\n","def initialize_weights(model):\n","    # Initializes weights according to the DCGAN paper\n","    for m in model.modules():\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n","            nn.init.normal_(m.weight.data, 0.0, 0.02)\n","\n","\n","def test():\n","    N, in_channels, H, W = 8, 3, 64, 64\n","    noise_dim = 100\n","    x = torch.randn((N, in_channels, H, W))\n","    disc = Discriminator(in_channels, 8)\n","    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n","    gen = Generator(noise_dim, in_channels, 8)\n","    z = torch.randn((N, noise_dim, 1, 1))\n","    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfSHARTSD-L3"},"outputs":[],"source":["def gradient_penalty(critic, labels, real, fake, device=\"cpu\"):\n","    BATCH_SIZE, C, H, W = real.shape\n","    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n","    interpolated_images = real * alpha + fake * (1 - alpha)\n","\n","    # Calculate critic scores\n","    mixed_scores = critic(interpolated_images, labels)\n","\n","    # Take the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(\n","        inputs=interpolated_images,\n","        outputs=mixed_scores,\n","        grad_outputs=torch.ones_like(mixed_scores),\n","        create_graph=True,\n","        retain_graph=True,\n","    )[0]\n","    gradient = gradient.view(gradient.shape[0], -1)\n","    gradient_norm = gradient.norm(2, dim=1)\n","    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n","    return gradient_penalty"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":739},"executionInfo":{"elapsed":6871,"status":"error","timestamp":1667791863829,"user":{"displayName":"Gokul Srinivasan","userId":"01072835723997484871"},"user_tz":300},"id":"DWbFkgfVZJhF","outputId":"3fe500d1-3514-4f6f-a060-87e1a862d656"},"outputs":[{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-5f7cd0b75abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#path for gen model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mgen_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/HAM10000/models/GAN.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# model.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1605\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tMissing key(s) in state_dict: \"net.4.weight\", \"net.4.bias\". \n\tUnexpected key(s) in state_dict: \"net.4.0.weight\", \"net.4.1.weight\", \"net.4.1.bias\", \"net.4.1.running_mean\", \"net.4.1.running_var\", \"net.4.1.num_batches_tracked\", \"net.5.weight\", \"net.5.bias\". \n\tsize mismatch for net.0.0.weight: copying a param with shape torch.Size([200, 512, 4, 4]) from checkpoint, the shape in current model is torch.Size([200, 256, 4, 4]).\n\tsize mismatch for net.0.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.0.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.0.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.0.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for net.1.0.weight: copying a param with shape torch.Size([512, 256, 4, 4]) from checkpoint, the shape in current model is torch.Size([256, 128, 4, 4]).\n\tsize mismatch for net.1.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for net.1.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for net.1.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for net.1.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for net.2.0.weight: copying a param with shape torch.Size([256, 128, 4, 4]) from checkpoint, the shape in current model is torch.Size([128, 64, 4, 4]).\n\tsize mismatch for net.2.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for net.2.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for net.2.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for net.2.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for net.3.0.weight: copying a param with shape torch.Size([128, 64, 4, 4]) from checkpoint, the shape in current model is torch.Size([64, 32, 4, 4]).\n\tsize mismatch for net.3.1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for net.3.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for net.3.1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for net.3.1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32])."]}],"source":["# Model class must be defined somewhere\n","import torchvision.transforms as transforms \n","\n","# Hyperparameters etc.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","LEARNING_RATE = 2e-4\n","BATCH_SIZE = 64\n","IMG_SIZE = 64\n","CHANNELS_IMG = 3\n","#new for the conditional gan\n","NUM_CLASSES = 7\n","GEN_EMBEDDING = 100\n","# end here \n","Z_DIM = 100\n","NUM_EPOCHS = 55 #test with this num (looks best at 32) maybe 80? 55? \n","FEATURES_CRITIC = 16\n","FEATURES_GEN = 16\n","CRITIC_ITERATIONS = 5\n","LAMBDA_GP = 10\n","\n","\n","# initialize gen and disc, note: discriminator should be called critic,\n","# according to WGAN paper (since it no longer outputs between [0, 1])\n","gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMG_SIZE, GEN_EMBEDDING).to(device)\n","critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC, NUM_CLASSES, IMG_SIZE).to(device)\n","\n","#path for gen model \n","gen_path = \"/content/drive/MyDrive/HAM10000/models/GAN.pt\"\n","gen.load_state_dict(torch.load(gen_path))\n","\n","# model.eval()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RUrDu_ozd6pF"},"outputs":[],"source":["# #create image \n","# noise = torch.randn(1, Z_DIM, 1, 1).to(device)\n","# label = torch.tensor([5]).to(device)\n","# fake = gen(noise, label)\n","\n","# #small test here\n","upscale = transforms.Compose([\n","    transforms.Resize(64*4),\n","    ])\n","# fake = upscale(fake)\n","# fake = torchvision.utils.make_grid(fake, normalize=True)\n","\n","# #save image\n","# img_path = \"/content/drive/MyDrive/HAM10000/GAN_Images/image.png\"\n","# save_image(fake, img_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SchR5HqMnMjr"},"outputs":[],"source":["def create_image(label, path):\n","  #create noise and label\n","  noise = torch.randn(1, Z_DIM, 1, 1).to(device)\n","  label = torch.tensor([label]).to(device)\n","  #produce image\n","  fake = gen(noise, label)\n","  #upscale image\n","  upscale = transforms.Compose([\n","      transforms.Resize(64*4),\n","      ])\n","  fake = upscale(fake)\n","  #normalize\n","  fake = torchvision.utils.make_grid(fake, normalize=True)\n","  #save image\n","  save_image(fake, str(\"/content/drive/MyDrive/HAM10000/GAN_Images_2/\" + path))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJAeV59lkkJd"},"outputs":[],"source":["gan_df = pd.DataFrame()\n","ids = []\n","dxs = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10UZ9N3Pmwtg"},"outputs":[],"source":["thresh = 100\n","# {'bkl': 0, 'nv': 1, 'df': 2, 'mel': 3, 'vasc': 4, 'bcc': 5, 'akiec': 6}\n","minority_classes = [0, 2, 3, 4, 5, 6]\n","image_ids = {} #map img_id ->  dx\n","#starting image = 0 \n","num = 0 \n","for c in minority_classes:\n","  for i in tqdm(range(0, thresh + 1)):\n","    #make the filename\n","    id = str(num).zfill(7)\n","    id = str(\"GAN_\" + id+\".png\")\n","    #make and save the image\n","    create_image(c, id)\n","    #add the information to the \n","    ids.append(id)\n","    dxs.append(c)\n","    num += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8ppXgW72dV0"},"outputs":[],"source":["gan_df[\"image_id\"] = ids\n","gan_df[\"dx\"] = dxs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IJ_Vl6c4S6D"},"outputs":[],"source":["gan_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43OToYuy1nrq"},"outputs":[],"source":["#forgot to save df \n","gan_df.to_csv(\"/content/drive/MyDrive/HAM10000/HAM10000_GAN_data_2.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mNczI8y0oGSQ"},"outputs":[],"source":["import os\n","os.chdir(\"/content/drive/MyDrive/HAM10000/GAN_Images_2/\")\n","!ls -1 | wc -l"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zYeVrnHhkbm"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMv4xbzcIzKlrV2jnIvZUOH","collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.7.9 64-bit ('3.7.9')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.9"},"vscode":{"interpreter":{"hash":"6779ff44cf17d229830241c018467467eabd002418a5e46f4b3ef249a8203865"}}},"nbformat":4,"nbformat_minor":0}
